{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate Model Retraining & Deployment Using the AWS Step Functions Data Science SDK\n",
    "\n",
    "**This sample is provided for demonstration purposes, make sure to conduct appropriate testing if derivating this code for your own use-cases!**\n",
    "\n",
    "This notebook describes how to use the AWS Step Functions Data Science SDK to create a machine learning model retraining workflow. The Step Functions SDK is an open source library that allows data scientists to easily create and execute machine learning workflows using AWS Step Functions and Amazon SageMaker. For more information, please see the following resources:\n",
    "* [AWS Step Functions](https://aws.amazon.com/step-functions/)\n",
    "* [AWS Step Functions Developer Guide](https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html)\n",
    "* [AWS Step Functions Data Science SDK](https://aws-step-functions-data-science-sdk.readthedocs.io)\n",
    "\n",
    "\n",
    "### Step 0: Get Admin Setup Results\n",
    "Bucket names, codecommit repo, docker image, IAM roles, ...\n",
    "\n",
    "In order to keep things orginized, we will save our `Source Code` (data processing, model training/serving scripts), `datasets`, as well as our trained `model(s) binaries` and their `test-performance metrics` all on S3, **versioned with respect to the date/time of each update.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade the stepfunctions library\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade stepfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from time import gmtime, strftime\n",
    "import logging\n",
    "import stepfunctions\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.steps.choice_rule import ChoiceRule\n",
    "from stepfunctions.steps import TrainingStep, ModelStep\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.workflow import Workflow\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)\n",
    "\n",
    "\n",
    "# Set project bucket, IAM Roles and Docker Image for Training\n",
    "with open('admin_setup.txt', 'r') as filehandle:\n",
    "    admin_setup = json.load(filehandle)\n",
    "\n",
    "SOURCE_DATA = admin_setup[\"raw_data_path\"]\n",
    "BUCKET = admin_setup[\"project_bucket\"]\n",
    "REPO_NAME = admin_setup[\"repo_name\"]\n",
    "TRAINING_IMAGE = admin_setup[\"docker_image\"]\n",
    "WORKFLOW_EXECUTION_ROLE = admin_setup[\"workflow_execution_role\"]\n",
    "\n",
    "\n",
    "# MLOps Hygiene\n",
    "WORKFLOW_NAME = \"my-project\"\n",
    "WORKFLOW_DATE_TIME = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "TRAINING_JOB_NAME = \"{}-{}\".format(WORKFLOW_NAME, WORKFLOW_DATE_TIME)\n",
    "\n",
    "SOURCE_CODE_PREFIX = \"{}/{}\".format(WORKFLOW_DATE_TIME, \"source-code\")\n",
    "SOURCE_CODE = \"s3://{}/{}/{}\".format(BUCKET, SOURCE_CODE_PREFIX, \"sourcedir.tar.gz\")\n",
    "\n",
    "OUTPUT_ARTIFACTS_PREFIX = \"{}/{}\".format(WORKFLOW_DATE_TIME, \"model-artifacts\")\n",
    "OUTPUT_ARTIFACTS_PATH = 's3://{}/{}'.format(BUCKET, WORKFLOW_DATE_TIME + '/model-artifacts/')#+\"/output/\"+\"model.tar.gz\"\n",
    "\n",
    "\n",
    "TRAINING_DATA_PATH = \"s3://{}/{}/data/train/train.csv\".format(BUCKET, WORKFLOW_DATE_TIME)\n",
    "VALIDATION_DATA_PATH = \"s3://{}/{}/data/validation/validation.csv\".format(BUCKET, WORKFLOW_DATE_TIME)\n",
    "TESTING_DATA_PATH = \"s3://{}/{}/data/test/test.csv\".format(BUCKET, WORKFLOW_DATE_TIME)\n",
    "\n",
    "codecommit_to_s3_event = {\n",
    "    \"s3BucketName\":BUCKET,\n",
    "    \"s3BucketKey\":\"{}/{}\".format(WORKFLOW_DATE_TIME, \"source-code\"),\n",
    "    \"repository\": REPO_NAME,\n",
    "    \"branch\": \"master\",\n",
    "    \"codecommitRegion\":\"us-east-1\",\n",
    "    \"repository_sagemaker_key\": \"sagemaker-train-serve-src\",\n",
    "    \"repository_sm_processing_key\": \"sagemaker-processing-src\"\n",
    "}\n",
    "\n",
    "processing_job_event = {\n",
    "    \"JOB_NAME\":\"{}-{}\".format(WORKFLOW_NAME, WORKFLOW_DATE_TIME),\n",
    "    \"BUCKET\": BUCKET,\n",
    "    \"WORKFLOW_DATE_TIME\":WORKFLOW_DATE_TIME,\n",
    "    \"SOURCE_CODE_PREFIX\":\"{}/{}\".format(WORKFLOW_DATE_TIME, \"source-code\"),\n",
    "    \"ENTRY_POINT_SCRIPT\":\"processing.py\",\n",
    "    \"TRAINING_IMAGE\":TRAINING_IMAGE,\n",
    "    \"ROLE_ARN\":WORKFLOW_EXECUTION_ROLE,\n",
    "    \"INSTANCE_TYPE\":\"ml.c5.xlarge\",\n",
    "    \"INSTANCE_COUNT\":1,\n",
    "    \"VOLUME_SIZE_GB\":10,\n",
    "    \"DATA_SOURCE\": SOURCE_DATA\n",
    "}\n",
    "\n",
    "training_job_event = {\n",
    "    \"TRAINING_JOB_NAME\":\"{}-{}\".format(WORKFLOW_NAME, WORKFLOW_DATE_TIME),\n",
    "    \"TRAINING_DATA\":TRAINING_DATA_PATH,\n",
    "    \"TESTING_DATA\":VALIDATION_DATA_PATH,\n",
    "    \"SOURCE_CODE\":\"s3://{}/{}/{}\".format(BUCKET, WORKFLOW_DATE_TIME, \"source-code/sourcedir.tar.gz\"),\n",
    "    \"ENTRY_POINT_SCRIPT\":\"train.py\",\n",
    "    \"TRAINING_IMAGE\":TRAINING_IMAGE,\n",
    "    \"ROLE_ARN\":WORKFLOW_EXECUTION_ROLE,\n",
    "    \"OUTPUT_ARTIFACTS_PATH\":\"s3://{}/{}/{}/\".format(BUCKET, WORKFLOW_DATE_TIME, \"model-artifacts\"),\n",
    "    \"INSTANCE_TYPE\":\"ml.c5.xlarge\",\n",
    "    \"INSTANCE_COUNT\":1,\n",
    "    \"VOLUME_SIZE_GB\":10,\n",
    "    \"PROCESSING_JOB_NAME\":\"{}-{}\".format(WORKFLOW_NAME, WORKFLOW_DATE_TIME)\n",
    "}\n",
    "\n",
    "deploy_event = {\n",
    "    \"EndPointConfigName\":\"{}-{}\".format(WORKFLOW_NAME, WORKFLOW_DATE_TIME),\n",
    "    \"EndPointName\":WORKFLOW_NAME,\n",
    "    \"ModelURL\":\"s3://{}/{}/{}\".format(BUCKET, WORKFLOW_DATE_TIME, \"model-artifacts/\"),\n",
    "    \"Directory\":\"s3://{}/{}/{}\".format(BUCKET, WORKFLOW_DATE_TIME, \"source-code/sourcedir.tar.gz\"),\n",
    "    \"Program\":\"train.py\",\n",
    "    \"Region\":\"us-east-1\",\n",
    "    \"TrainingImage\":TRAINING_IMAGE,\n",
    "    \"ROLE_ARN\":WORKFLOW_EXECUTION_ROLE,\n",
    "    \"OUTPUT_ARTIFACTS_PATH\":\"s3://{}/{}/{}/{}\".format(BUCKET, WORKFLOW_DATE_TIME, \"model-artifacts\", \"{}-{}\".format(WORKFLOW_NAME, WORKFLOW_DATE_TIME)+\"/output/model.tar.gz\"),\n",
    "    \"DeploymentInstanceType\":\"ml.c5.xlarge\",\n",
    "    \"DeploymentInstanceCount\":1\n",
    "}\n",
    "\n",
    "\n",
    "processing_status_event = {\n",
    "    'JOB_NAME':\"{}-{}\".format(WORKFLOW_NAME, WORKFLOW_DATE_TIME)\n",
    "}\n",
    "training_status_event = {\n",
    "    'JOB_NAME':\"{}-{}\".format(WORKFLOW_NAME, WORKFLOW_DATE_TIME)\n",
    "}\n",
    "\n",
    "model_accuracy_event = {\n",
    "    'TrainingJobName':\"{}-{}\".format(WORKFLOW_NAME, WORKFLOW_DATE_TIME)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Wrokflow Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_input = ExecutionInput(schema={\n",
    "    'CodeCommitToS3Step': str,\n",
    "    'DataProcessingStep': str,\n",
    "    'DataProcessingStatusStep': str,\n",
    "    'TrainingStep': str,\n",
    "    'TrainingStatusStep': str,\n",
    "    'ModelAccuracyStep': str,\n",
    "    'DeployModelStep': str\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Workflow States (steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StepN: Create Fail State\n",
    "fail_step = steps.states.Fail(\n",
    "    'Workflow Failed',\n",
    "    comment='Either Validation accuracy is lower than threshold or one of processing, training, deployment jobs has faild.'\n",
    ")\n",
    "\n",
    "# Step1: Copy source code from CodeCommit to S3\n",
    "codecommit_to_s3_step = steps.compute.LambdaStep(\n",
    "    state_id = 'Put Code on S3',\n",
    "    parameters={ \n",
    "        \"FunctionName\": execution_input['CodeCommitToS3Step'],\n",
    "        'Payload':codecommit_to_s3_event\n",
    "    }\n",
    ")\n",
    "# Step2: Run SageMaker Data Processing Job\n",
    "data_processing_step = steps.compute.LambdaStep(\n",
    "    state_id = 'Data Processing',\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['DataProcessingStep'],\n",
    "        'Payload':processing_job_event\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step3: Wait a little bit\n",
    "wait_for_data_processing = steps.states.Wait(\n",
    "    state_id = \"Wait 30 Seconds\",\n",
    "    seconds = 30\n",
    ")\n",
    "\n",
    "# Step4: Check if processing job has finished\n",
    "get_processing_status = steps.compute.LambdaStep(\n",
    "    state_id = \"Processing Job Status\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['DataProcessingStatusStep'],\n",
    "        'Payload':processing_status_event\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step5: If processing job is not done, go back to waiting (Step3), if done go to Step6, else go to failure\n",
    "# We will author this step later\n",
    "# ...\n",
    "\n",
    "# Step6: Start SageMaker Training Job\n",
    "model_training_step = steps.compute.LambdaStep(\n",
    "    'Model Training',\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['TrainingStep'],\n",
    "        'Payload':training_job_event\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step7: Wait a little bit\n",
    "wait_for_training = steps.states.Wait(\n",
    "    state_id = \"Wait 60 Seconds\",\n",
    "    seconds = 60\n",
    ")\n",
    "\n",
    "# Step8: Check if training job has finished\n",
    "get_training_status = steps.compute.LambdaStep(\n",
    "    state_id = \"Training Job Status\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['TrainingStatusStep'],\n",
    "        'Payload':training_status_event\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step9: If training job is not done, go back to waiting (Step7), if done go to Step10, else go to failure\n",
    "# We will author this step later\n",
    "# ...\n",
    "\n",
    "# Step10: Get model accuracy (custom print to logs during training)\n",
    "get_model_accuracy = steps.compute.LambdaStep(\n",
    "    state_id = \"Get Model Median Abs. Err.\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['ModelAccuracyStep'],\n",
    "        'Payload':model_accuracy_event\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step11: If model's Median Abs. Err. is less than 2, go back to next step (deployment), else go to failure\n",
    "# We will author this step later\n",
    "# ...\n",
    "\n",
    "# Step12: Create Endpoint (or update it if it exists)\n",
    "deploy_model_step = steps.compute.LambdaStep(\n",
    "    'Deploy Model',\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['DeployModelStep'],\n",
    "        'Payload':deploy_event\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step5: If processing job is not done, go back to waiting (Step3), if done go to Step6, else go to failure\n",
    "check_pocessing_status = steps.states.Choice(\n",
    "    state_id = \"Processing Job Complete?\",\n",
    ")\n",
    "\n",
    "processing_job_output = get_processing_status.output()\n",
    "\n",
    "completed_rule = ChoiceRule.StringEquals(variable=processing_job_output['Payload']['ProcessingJobStatus'],\n",
    "                                    value=\"Completed\"\n",
    "                                   )\n",
    "in_progress_rule = ChoiceRule.StringEquals(variable=processing_job_output['Payload']['ProcessingJobStatus'],\n",
    "                                           value=\"InProgress\"\n",
    "                                          )\n",
    "\n",
    "check_pocessing_status.add_choice(rule=completed_rule, next_step=model_training_step)\n",
    "check_pocessing_status.add_choice(rule=in_progress_rule, next_step=wait_for_data_processing)\n",
    "check_pocessing_status.default_choice(fail_step)\n",
    "\n",
    "\n",
    "# Step9: If training job is not done, go back to waiting (Step7), if done go to Step10, else go to failure\n",
    "check_training_status = steps.states.Choice(\n",
    "    state_id = \"Training Job Complete?\",\n",
    ")\n",
    "\n",
    "training_job_output = get_training_status.output()\n",
    "\n",
    "completed_rule = ChoiceRule.StringEquals(variable=training_job_output['Payload']['TrainingJobStatus'],\n",
    "                                    value=\"Completed\"\n",
    "                                   )\n",
    "in_progress_rule = ChoiceRule.StringEquals(variable=training_job_output['Payload']['TrainingJobStatus'],\n",
    "                                           value=\"InProgress\"\n",
    "                                          )\n",
    "\n",
    "check_training_status.add_choice(rule=completed_rule, next_step=get_model_accuracy)\n",
    "check_training_status.add_choice(rule=in_progress_rule, next_step=wait_for_training)\n",
    "check_training_status.default_choice(fail_step)\n",
    "\n",
    "\n",
    "# Step11: If model's Median Abs. Err. is less than 2, go back to next step (deployment), else go to failure\n",
    "check_accuracy_step = steps.states.Choice(\n",
    "    'Median-AE < 3'\n",
    ")\n",
    "\n",
    "threshold_rule = ChoiceRule.NumericLessThan(variable=get_training_status.output()['Payload']['trainingMetrics'][0]['Value'], value=3)\n",
    "\n",
    "check_accuracy_step.add_choice(rule=threshold_rule, next_step=deploy_model_step)\n",
    "\n",
    "check_accuracy_step.default_choice(next_step=fail_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link all the Steps Together\n",
    "We create a workflow definition by chaining all of the steps together that we've created. See [Chain](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.states.Chain) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain Steps 6-12\n",
    "model_training_step.next(wait_for_training)\n",
    "wait_for_training.next(get_training_status)\n",
    "get_training_status.next(check_training_status)\n",
    "get_model_accuracy.next(check_accuracy_step)\n",
    "\n",
    "# Chain the whole workflow\n",
    "workflow_definition = steps.Chain([\n",
    "    codecommit_to_s3_step,\n",
    "    data_processing_step,\n",
    "    wait_for_data_processing,\n",
    "    get_processing_status,\n",
    "    check_pocessing_status\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Workflow\n",
    "Create your workflow using the workflow definition above, and render the graph with [render_graph](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.render_graph):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://do0of8uwbahzz.cloudfront.net/graph.css\">\n",
       "<div id=\"graph-959\" class=\"workflowgraph\">\n",
       "    \n",
       "    <svg></svg>\n",
       "    \n",
       "</div>\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "require.config({\n",
       "    paths: {\n",
       "        sfn: \"https://do0of8uwbahzz.cloudfront.net/sfn\",\n",
       "    }\n",
       "});\n",
       "\n",
       "require(['sfn'], function(sfn) {\n",
       "    var element = document.getElementById('graph-959')\n",
       "\n",
       "    var options = {\n",
       "        width: parseFloat(getComputedStyle(element, null).width.replace(\"px\", \"\")),\n",
       "        height: 600,\n",
       "        layout: 'LR',\n",
       "        resizeHeight: true\n",
       "    };\n",
       "\n",
       "    var definition = {\"StartAt\": \"Put Code on S3\", \"States\": {\"Put Code on S3\": {\"Parameters\": {\"FunctionName.$\": \"$$.Execution.Input['CodeCommitToS3Step']\", \"Payload\": {\"s3BucketName\": \"my-project-227921966468\", \"s3BucketKey\": \"2020-06-10-21-34-27/source-code\", \"repository\": \"my-project\", \"branch\": \"master\", \"codecommitRegion\": \"us-east-1\", \"repository_sagemaker_key\": \"sagemaker-train-serve-src\", \"repository_sm_processing_key\": \"sagemaker-processing-src\"}}, \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"Next\": \"Data Processing\"}, \"Data Processing\": {\"Parameters\": {\"FunctionName.$\": \"$$.Execution.Input['DataProcessingStep']\", \"Payload\": {\"JOB_NAME\": \"my-project-2020-06-10-21-34-27\", \"BUCKET\": \"my-project-227921966468\", \"WORKFLOW_DATE_TIME\": \"2020-06-10-21-34-27\", \"SOURCE_CODE_PREFIX\": \"2020-06-10-21-34-27/source-code\", \"ENTRY_POINT_SCRIPT\": \"processing.py\", \"TRAINING_IMAGE\": \"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3\", \"ROLE_ARN\": \"arn:aws:iam::227921966468:role/My-StepFunction-Workflow-Role\", \"INSTANCE_TYPE\": \"ml.c5.xlarge\", \"INSTANCE_COUNT\": 1, \"VOLUME_SIZE_GB\": 10, \"DATA_SOURCE\": \"s3://my-datalake-227921966468/data/boston.csv\"}}, \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"Next\": \"Wait 30 Seconds\"}, \"Wait 30 Seconds\": {\"Seconds\": 30, \"Type\": \"Wait\", \"Next\": \"Processing Job Status\"}, \"Processing Job Status\": {\"Parameters\": {\"FunctionName.$\": \"$$.Execution.Input['DataProcessingStatusStep']\", \"Payload\": {\"JOB_NAME\": \"my-project-2020-06-10-21-34-27\"}}, \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"Next\": \"Processing Job Complete?\"}, \"Processing Job Complete?\": {\"Type\": \"Choice\", \"Choices\": [{\"Variable\": \"$['Payload']['ProcessingJobStatus']\", \"StringEquals\": \"Completed\", \"Next\": \"Model Training\"}, {\"Variable\": \"$['Payload']['ProcessingJobStatus']\", \"StringEquals\": \"InProgress\", \"Next\": \"Wait 30 Seconds\"}], \"Default\": \"Workflow Failed\"}, \"Workflow Failed\": {\"Comment\": \"Either Validation accuracy is lower than threshold or one of processing, training, deployment jobs has faild.\", \"Type\": \"Fail\"}, \"Model Training\": {\"Parameters\": {\"FunctionName.$\": \"$$.Execution.Input['TrainingStep']\", \"Payload\": {\"TRAINING_JOB_NAME\": \"my-project-2020-06-10-21-34-27\", \"TRAINING_DATA\": \"s3://my-project-227921966468/2020-06-10-21-34-27/data/train/train.csv\", \"TESTING_DATA\": \"s3://my-project-227921966468/2020-06-10-21-34-27/data/validation/validation.csv\", \"SOURCE_CODE\": \"s3://my-project-227921966468/2020-06-10-21-34-27/source-code/sourcedir.tar.gz\", \"ENTRY_POINT_SCRIPT\": \"train.py\", \"TRAINING_IMAGE\": \"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3\", \"ROLE_ARN\": \"arn:aws:iam::227921966468:role/My-StepFunction-Workflow-Role\", \"OUTPUT_ARTIFACTS_PATH\": \"s3://my-project-227921966468/2020-06-10-21-34-27/model-artifacts/\", \"INSTANCE_TYPE\": \"ml.c5.xlarge\", \"INSTANCE_COUNT\": 1, \"VOLUME_SIZE_GB\": 10, \"PROCESSING_JOB_NAME\": \"my-project-2020-06-10-21-34-27\"}}, \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"Next\": \"Wait 60 Seconds\"}, \"Wait 60 Seconds\": {\"Seconds\": 60, \"Type\": \"Wait\", \"Next\": \"Training Job Status\"}, \"Training Job Status\": {\"Parameters\": {\"FunctionName.$\": \"$$.Execution.Input['TrainingStatusStep']\", \"Payload\": {\"JOB_NAME\": \"my-project-2020-06-10-21-34-27\"}}, \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"Next\": \"Training Job Complete?\"}, \"Training Job Complete?\": {\"Type\": \"Choice\", \"Choices\": [{\"Variable\": \"$['Payload']['TrainingJobStatus']\", \"StringEquals\": \"Completed\", \"Next\": \"Get Model Median Abs. Err.\"}, {\"Variable\": \"$['Payload']['TrainingJobStatus']\", \"StringEquals\": \"InProgress\", \"Next\": \"Wait 60 Seconds\"}], \"Default\": \"Workflow Failed\"}, \"Get Model Median Abs. Err.\": {\"Parameters\": {\"FunctionName.$\": \"$$.Execution.Input['ModelAccuracyStep']\", \"Payload\": {\"TrainingJobName\": \"my-project-2020-06-10-21-34-27\"}}, \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"Next\": \"Median-AE < 3\"}, \"Median-AE < 3\": {\"Type\": \"Choice\", \"Choices\": [{\"Variable\": \"$['Payload']['trainingMetrics'][0]['Value']\", \"NumericLessThan\": 3, \"Next\": \"Deploy Model\"}], \"Default\": \"Workflow Failed\"}, \"Deploy Model\": {\"Parameters\": {\"FunctionName.$\": \"$$.Execution.Input['DeployModelStep']\", \"Payload\": {\"EndPointConfigName\": \"my-project-2020-06-10-21-34-27\", \"EndPointName\": \"my-project\", \"ModelURL\": \"s3://my-project-227921966468/2020-06-10-21-34-27/model-artifacts/\", \"Directory\": \"s3://my-project-227921966468/2020-06-10-21-34-27/source-code/sourcedir.tar.gz\", \"Program\": \"train.py\", \"Region\": \"us-east-1\", \"TrainingImage\": \"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3\", \"ROLE_ARN\": \"arn:aws:iam::227921966468:role/My-StepFunction-Workflow-Role\", \"OUTPUT_ARTIFACTS_PATH\": \"s3://my-project-227921966468/2020-06-10-21-34-27/model-artifacts/my-project-2020-06-10-21-34-27/output/model.tar.gz\", \"DeploymentInstanceType\": \"ml.c5.xlarge\", \"DeploymentInstanceCount\": 1}}, \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"End\": true}}};\n",
       "    var elementId = '#graph-959';\n",
       "\n",
       "    var graph = new sfn.StateMachineGraph(definition, elementId, options);\n",
       "    graph.render();\n",
       "});\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = Workflow(\n",
    "    name=WORKFLOW_NAME,\n",
    "    definition=workflow_definition,\n",
    "    role=WORKFLOW_EXECUTION_ROLE,\n",
    "    execution_input=execution_input\n",
    ")\n",
    "workflow.render_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the workflow in AWS Step Functions with [create](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.create):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the workflow with [execute](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.execute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = workflow.execute(\n",
    "    inputs={\n",
    "        'CodeCommitToS3Step': WORKFLOW_NAME + '-codecommit-to-s3',\n",
    "        'DataProcessingStep': WORKFLOW_NAME + '-create-sagemaker-prcoessing-job',\n",
    "        'DataProcessingStatusStep': WORKFLOW_NAME + '-query-data-processing-status',\n",
    "        'TrainingStep': WORKFLOW_NAME + '-create-sagemaker-training-job',\n",
    "        'TrainingStatusStep': WORKFLOW_NAME + '-query-training-status',\n",
    "        'ModelAccuracyStep': WORKFLOW_NAME + '-query-model-accuracy',\n",
    "        'DeployModelStep': WORKFLOW_NAME + '-deploy-sagemaker-model-job'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render workflow progress with the [render_progress](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Execution.render_progress). This generates a snapshot of the current state of your workflow as it executes. This is a static image therefore you must run the cell again to check progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.render_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_events(html=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
